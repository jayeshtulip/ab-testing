"""FastAPI application for loan default prediction service."""

import os
import time
import asyncio
from contextlib import asynccontextmanager
from typing import Dict, Any, List

import uvicorn
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import mlflow.sklearn
import joblib
import numpy as np
import pandas as pd
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import logging

from src.config.settings import get_settings
from src.utils.logger import get_logger
from src.api.schemas.requests import PredictionRequest, BatchPredictionRequest
from src.api.schemas.responses import PredictionResponse, BatchPredictionResponse, HealthResponse
from src.api.middleware.metrics import PrometheusMiddleware
from src.api.middleware.logging import LoggingMiddleware

logger = get_logger(__name__)
settings = get_settings()

# Prometheus metrics
PREDICTION_COUNTER = Counter('loan_predictions_total', 'Total number of predictions', ['model_version', 'prediction'])
PREDICTION_LATENCY = Histogram('loan_prediction_duration_seconds', 'Prediction latency')
MODEL_SCORE_HISTOGRAM = Histogram('loan_model_score', 'Distribution of model prediction scores',
                                 buckets=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
ERROR_COUNTER = Counter('loan_api_errors_total', 'Total number of API errors', ['error_type'])
ACTIVE_REQUESTS = Gauge('loan_active_requests', 'Number of active requests')

# Global model storage
model_cache = {
    "model": None,
    "preprocessor": None,
    "label_encoder": None,
    "model_version": None,
    "last_loaded": None
}


class ModelManager:
    """Manages model loading and caching."""
    
    def __init__(self):
        self.cache_ttl = settings.get_monitoring_params().get("model_cache_ttl", 3600)
    
    async def load_latest_model(self):
        """Load the latest model from MLflow registry."""
        try:
            logger.info("Loading latest model from MLflow...")
            
            # Load model from MLflow registry
            client = mlflow.tracking.MlflowClient()
            latest_version = client.get_latest_versions(
                settings.mlflow.model_name, 
                stages=["Production", "Staging", "None"]
            )[0]
            
            model_uri = f"models:/{settings.mlflow.model_name}/{latest_version.version}"
            model = mlflow.sklearn.load_model(model_uri)
            
            # Load preprocessor and label encoder
            models_path = settings.model_paths["models"]
            preprocessor = joblib.load(models_path / "preprocessor.joblib")
            
            label_encoder = None
            if (models_path / "label_encoder.joblib").exists():
                label_encoder = joblib.load(models_path / "label_encoder.joblib")
            
            # Update cache
            model_cache.update({
                "model": model,
                "preprocessor": preprocessor,
                "label_encoder": label_encoder,
                "model_version": latest_version.version,
                "last_loaded": time.time()
            })
            
            logger.info(f"Model loaded successfully. Version: {latest_version.version}")
            
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            ERROR_COUNTER.labels(error_type="model_loading").inc()
            raise
    
    async def get_model(self):
        """Get cached model or load if needed."""
        # Check if model needs reloading
        if (model_cache["model"] is None or 
            model_cache["last_loaded"] is None or
            time.time() - model_cache["last_loaded"] > self.cache_ttl):
            
            await self.load_latest_model()
        
        return model_cache


model_manager = ModelManager()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management."""
    # Startup
    logger.info("Starting up application...")
    
    # Start Prometheus metrics server
    metrics_port = settings.monitoring.metrics_port
    start_http_server(metrics_port)
    logger.info(f"Prometheus metrics server started on port {metrics_port}")
    
    # Load initial model
    try:
        await model_manager.load_latest_model()
    except Exception as e:
        logger.error(f"Failed to load initial model: {e}")
    
    yield
    
    # Shutdown
    logger.info("Shutting down application...")


# Create FastAPI app
app = FastAPI(
    title="Loan Default Prediction API",
    description="Production-ready API for loan default prediction",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Add middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["*"]  # Configure appropriately for production
)

app.add_middleware(PrometheusMiddleware)
app.add_middleware(LoggingMiddleware)


@app.get("/", response_model=Dict[str, str])
async def root():
    """Root endpoint."""
    return {
        "message": "Loan Default Prediction API",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    try:
        # Check model availability
        model_data = await model_manager.get_model()
        model_available = model_data["model"] is not None
        
        # Check MLflow connection
        mlflow_healthy = True
        try:
            mlflow.tracking.MlflowClient().list_experiments()
        except Exception:
            mlflow_healthy = False
        
        status = "healthy" if model_available and mlflow_healthy else "unhealthy"
        
        return HealthResponse(
            status=status,
            timestamp=time.time(),
            model_available=model_available,
            model_version=model_data.get("model_version"),
            mlflow_healthy=mlflow_healthy
        )
        
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        ERROR_COUNTER.labels(error_type="health_check").inc()
        raise HTTPException(status_code=500, detail="Health check failed")


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Single prediction endpoint."""
    ACTIVE_REQUESTS.inc()
    
    try:
        with PREDICTION_LATENCY.time():
            # Get model
            model_data = await model_manager.get_model()
            
            if model_data["model"] is None:
                ERROR_COUNTER.labels(error_type="model_not_available").inc()
                raise HTTPException(status_code=503, detail="Model not available")
            
            # Prepare input data
            input_df = pd.DataFrame([request.features])
            
            # Preprocess
            X_processed = model_data["preprocessor"].transform(input_df)
            
            # Make prediction
            prediction_proba = model_data["model"].predict_proba(X_processed)[0]
            prediction_class = model_data["model"].predict(X_processed)[0]
            
            # Convert back to original labels if label encoder exists
            if model_data["label_encoder"] is not None:
                prediction_label = model_data["label_encoder"].inverse_transform([prediction_class])[0]
            else:
                prediction_label = str(prediction_class)
            
            # Get probability for positive class (default risk)
            risk_probability = float(prediction_proba[1]) if len(prediction_proba) > 1 else float(prediction_proba[0])
            
            # Update metrics
            PREDICTION_COUNTER.labels(
                model_version=model_data["model_version"], 
                prediction=prediction_label
            ).inc()
            MODEL_SCORE_HISTOGRAM.observe(risk_probability)
            
            response = PredictionResponse(
                prediction=prediction_label,
                probability=risk_probability,
                model_version=model_data["model_version"],
                timestamp=time.time()
            )
            
            logger.info(f"Prediction made: {prediction_label} (prob: {risk_probability:.4f})")
            
            return response
            
    except Exception as e:
        ERROR_COUNTER.labels(error_type="prediction_error").inc()
        logger.error(f"Prediction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")
    
    finally:
        ACTIVE_REQUESTS.dec()


@app.post("/predict/batch", response_model=BatchPredictionResponse)
async def predict_batch(request: BatchPredictionRequest):
    """Batch prediction endpoint."""
    ACTIVE_REQUESTS.inc()
    
    try:
        # Check batch size limit
        max_batch_size = settings.get_monitoring_params().get("batch_prediction_max_size", 1000)
        if len(request.instances) > max_batch_size:
            raise HTTPException(
                status_code=400, 
                detail=f"Batch size {len(request.instances)} exceeds maximum {max_batch_size}"
            )
        
        with PREDICTION_LATENCY.time():
            # Get model
            model_data = await model_manager.get_model()
            
            if model_data["model"] is None:
                ERROR_COUNTER.labels(error_type="model_not_available").inc()
                raise HTTPException(status_code=503, detail="Model not available")
            
            # Prepare batch data
            input_df = pd.DataFrame([instance.features for instance in request.instances])
            
            # Preprocess
            X_processed = model_data["preprocessor"].transform(input_df)
            
            # Make predictions
            predictions_proba = model_data["model"].predict_proba(X_processed)
            predictions_class = model_data["model"].predict(X_processed)
            
            # Process results
            results = []
            for i, (proba, pred_class) in enumerate(zip(predictions_proba, predictions_class)):
                # Convert to original labels if needed
                if model_data["label_encoder"] is not None:
                    pred_label = model_data["label_encoder"].inverse_transform([pred_class])[0]
                else:
                    pred_label = str(pred_class)
                
                risk_probability = float(proba[1]) if len(proba) > 1 else float(proba[0])
                
                results.append(PredictionResponse(
                    prediction=pred_label,
                    probability=risk_probability,
                    model_version=model_data["model_version"],
                    timestamp=time.time()
                ))
                
                # Update metrics
                PREDICTION_COUNTER.labels(
                    model_version=model_data["model_version"], 
                    prediction=pred_label
                ).inc()
                MODEL_SCORE_HISTOGRAM.observe(risk_probability)
            
            logger.info(f"Batch prediction completed: {len(results)} predictions")
            
            return BatchPredictionResponse(
                predictions=results,
                batch_size=len(results),
                model_version=model_data["model_version"],
                timestamp=time.time()
            )
            
    except Exception as e:
        ERROR_COUNTER.labels(error_type="batch_prediction_error").inc()
        logger.error(f"Batch prediction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Batch prediction failed: {str(e)}")
    
    finally:
        ACTIVE_REQUESTS.dec()


@app.get("/model/info")
async def get_model_info():
    """Get current model information."""
    try:
        model_data = await model_manager.get_model()
        
        if model_data["model"] is None:
            raise HTTPException(status_code=503, detail="Model not available")
        
        # Get model metadata
        models_path = settings.model_paths["models"]
        metadata_path = models_path / "model_metadata.json"
        
        metadata = {}
        if metadata_path.exists():
            import json
            with open(metadata_path, "r") as f:
                metadata = json.load(f)
        
        return {
            "model_version": model_data["model_version"],
            "model_type": type(model_data["model"]).__name__,
            "last_loaded": model_data["last_loaded"],
            "feature_count": getattr(model_data["model"], "n_features_in_", None),
            "metadata": metadata
        }
        
    except Exception as e:
        logger.error(f"Failed to get model info: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get model info")


@app.post("/model/reload")
async def reload_model(background_tasks: BackgroundTasks):
    """Reload model from registry."""
    try:
        background_tasks.add_task(model_manager.load_latest_model)
        return {"message": "Model reload initiated"}
        
    except Exception as e:
        logger.error(f"Failed to reload model: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to reload model")


@app.get("/metrics")
async def get_metrics():
    """Get application metrics (Prometheus format)."""
    # This endpoint is handled by the Prometheus client
    # Access metrics at /metrics endpoint directly
    return {"message": "Metrics available at /metrics endpoint"}


def main():
    """Main function to run the API server."""
    uvicorn.run(
        "src.api.main:app",
        host=settings.api.host,
        port=settings.api.port,
        workers=1,  # Use 1 worker for development, increase for production
        log_level=settings.api.log_level,
        reload=settings.is_development
    )


if __name__ == "__main__":
    main()