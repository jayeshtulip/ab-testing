name: Model Monitoring

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  workflow_dispatch:
    inputs:
      check_type:
        description: 'Type of monitoring check'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - performance
        - drift
        - infrastructure
      environment:
        description: 'Environment to monitor'
        required: true
        default: 'production'
        type: choice
        options:
        - production
        - staging

env:
  PYTHON_VERSION: '3.10'
  MLFLOW_TRACKING_URI: http://ab124afa4840a4f8298398f9c7fd7c7e-306571921.ap-south-1.elb.amazonaws.com
  AWS_REGION: ap-south-1
  GRAFANA_URL: http://grafana.loan-default.local  # We'll set this up
  PROMETHEUS_URL: http://prometheus.loan-default.local  # We'll set this up

jobs:
  # INFRASTRUCTURE HEALTH MONITORING
  infrastructure-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type == 'infrastructure' || github.event.inputs.check_type == 'all' || github.event_name == 'schedule'
    outputs:
      cluster-healthy: ${{ steps.health.outputs.healthy }}
      pod-metrics: ${{ steps.health.outputs.metrics }}
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
    
    - name: üêç Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: üì¶ Install monitoring dependencies
      run: |
        pip install requests pandas numpy prometheus-client boto3 psutil
    
    - name: ‚öôÔ∏è Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: üîß Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: ‚öôÔ∏è Configure kubectl for EKS
      run: |
        aws eks update-kubeconfig --region $AWS_REGION --name loan-eks-simple
    
    - name: ü©∫ Check infrastructure health
      id: health
      env:
        NAMESPACE: ${{ github.event.inputs.environment == 'staging' && 'loan-default-staging' || 'loan-default' }}
      run: |
        python << 'EOF'
        import subprocess
        import json
        import os
        import sys
        from datetime import datetime
        
        namespace = os.environ['NAMESPACE']
        print(f"üîç Monitoring infrastructure health for {namespace}")
        
        # Check pod health
        try:
            pods_result = subprocess.run([
                'kubectl', 'get', 'pods', '-n', namespace, 
                '-l', 'app=loan-default-api', '-o', 'json'
            ], capture_output=True, text=True, check=True)
            
            pods_data = json.loads(pods_result.stdout)
            pods = pods_data['items']
            
            healthy_pods = 0
            total_pods = len(pods)
            pod_metrics = []
            
            for pod in pods:
                pod_name = pod['metadata']['name']
                status = pod['status']['phase']
                ready = all(condition['status'] == 'True' 
                           for condition in pod['status'].get('conditions', [])
                           if condition['type'] == 'Ready')
                
                if status == 'Running' and ready:
                    healthy_pods += 1
                
                # Get resource usage
                try:
                    top_result = subprocess.run([
                        'kubectl', 'top', 'pod', pod_name, '-n', namespace
                    ], capture_output=True, text=True)
                    
                    if top_result.returncode == 0:
                        lines = top_result.stdout.strip().split('\n')
                        if len(lines) > 1:
                            parts = lines[1].split()
                            cpu_usage = parts[1]
                            memory_usage = parts[2]
                        else:
                            cpu_usage = "N/A"
                            memory_usage = "N/A"
                    else:
                        cpu_usage = "N/A"
                        memory_usage = "N/A"
                except:
                    cpu_usage = "N/A"
                    memory_usage = "N/A"
                
                pod_metrics.append({
                    'name': pod_name,
                    'status': status,
                    'ready': ready,
                    'cpu': cpu_usage,
                    'memory': memory_usage
                })
            
            # Check service endpoints
            endpoints_result = subprocess.run([
                'kubectl', 'get', 'endpoints', 'loan-default-api-service', 
                '-n', namespace, '-o', 'json'
            ], capture_output=True, text=True)
            
            service_healthy = False
            if endpoints_result.returncode == 0:
                endpoints_data = json.loads(endpoints_result.stdout)
                subsets = endpoints_data.get('subsets', [])
                if subsets and subsets[0].get('addresses'):
                    service_healthy = True
            
            # Overall health assessment
            health_percentage = (healthy_pods / total_pods * 100) if total_pods > 0 else 0
            overall_healthy = health_percentage >= 66 and service_healthy  # At least 2/3 pods healthy
            
            print(f"üìä Infrastructure Health Report:")
            print(f"   Healthy Pods: {healthy_pods}/{total_pods} ({health_percentage:.1f}%)")
            print(f"   Service Endpoints: {'‚úÖ Healthy' if service_healthy else '‚ùå Unhealthy'}")
            print(f"   Overall Status: {'‚úÖ Healthy' if overall_healthy else '‚ùå Unhealthy'}")
            
            # Output for next jobs
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"healthy={str(overall_healthy).lower()}\n")
                f.write(f"metrics={json.dumps(pod_metrics)}\n")
            
            if not overall_healthy:
                print("üö® Infrastructure health check failed!")
                sys.exit(1)
                
        except Exception as e:
            print(f"üí• Infrastructure health check error: {e}")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write("healthy=false\n")
                f.write("metrics=[]\n")
            sys.exit(1)
        EOF

  # MODEL PERFORMANCE MONITORING
  model-performance:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type == 'performance' || github.event.inputs.check_type == 'all' || github.event_name == 'schedule'
    outputs:
      performance-score: ${{ steps.perf.outputs.score }}
      alerts: ${{ steps.perf.outputs.alerts }}
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
    
    - name: üêç Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: üì¶ Install dependencies
      run: |
        pip install mlflow pandas numpy scikit-learn requests matplotlib seaborn
    
    - name: üìä Monitor model performance
      id: perf
      env:
        API_URL: http://a015d0a5e673c47e9b4ff468a0af8419-1590493237.ap-south-1.elb.amazonaws.com
      run: |
        python << 'EOF'
        import mlflow
        import requests
        import pandas as pd
        import numpy as np
        import json
        import os
        from datetime import datetime, timedelta
        
        print("üìä Starting model performance monitoring...")
        
        # Set MLflow tracking URI
        mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
        client = mlflow.MlflowClient()
        
        alerts = []
        
        try:
            # Get current model info
            api_url = os.environ['API_URL']
            model_info_response = requests.get(f"{api_url}/model/info", timeout=10)
            
            if model_info_response.status_code == 200:
                model_info = model_info_response.json()
                print(f"üì¶ Current model: {model_info}")
            else:
                alerts.append("‚ùå Could not retrieve model info from API")
            
            # Test API health and response time
            start_time = datetime.now()
            health_response = requests.get(f"{api_url}/health", timeout=10)
            response_time = (datetime.now() - start_time).total_seconds()
            
            if health_response.status_code == 200:
                print(f"‚úÖ API health check passed (response time: {response_time:.2f}s)")
                if response_time > 2.0:
                    alerts.append(f"‚ö†Ô∏è API response time high: {response_time:.2f}s")
            else:
                alerts.append("‚ùå API health check failed")
            
            # Test prediction endpoint
            test_features = [1.0] * 20  # Sample features
            pred_start = datetime.now()
            pred_response = requests.post(
                f"{api_url}/predict",
                json={"features": test_features},
                timeout=10
            )
            pred_time = (datetime.now() - pred_start).total_seconds()
            
            if pred_response.status_code == 200:
                pred_result = pred_response.json()
                print(f"‚úÖ Prediction test passed (time: {pred_time:.2f}s)")
                print(f"   Prediction: {pred_result['prediction']}")
                print(f"   Probability: {pred_result['probability']:.4f}")
                
                if pred_time > 1.0:
                    alerts.append(f"‚ö†Ô∏è Prediction latency high: {pred_time:.2f}s")
            else:
                alerts.append("‚ùå Prediction endpoint failed")
            
            # Get latest model metrics from MLflow
            try:
                model_name = "loan-default-model"
                latest_versions = client.get_latest_versions(model_name, stages=["Production", "Staging", "None"])
                
                if latest_versions:
                    latest_model = latest_versions[0]
                    run = client.get_run(latest_model.run_id)
                    metrics = run.data.metrics
                    
                    print(f"üìà Latest model metrics:")
                    for metric, value in metrics.items():
                        print(f"   {metric}: {value:.4f}")
                    
                    # Performance thresholds
                    if metrics.get('accuracy', 0) < 0.70:
                        alerts.append(f"‚ö†Ô∏è Model accuracy below threshold: {metrics.get('accuracy', 0):.4f}")
                    
                    if metrics.get('f1_score', 0) < 0.30:  # Using relaxed threshold
                        alerts.append(f"‚ö†Ô∏è Model F1-score below threshold: {metrics.get('f1_score', 0):.4f}")
                
            except Exception as e:
                alerts.append(f"‚ùå Could not retrieve model metrics: {str(e)}")
            
            # Calculate overall performance score
            base_score = 100
            score_deductions = len(alerts) * 15  # 15 points per alert
            performance_score = max(0, base_score - score_deductions)
            
            print(f"üìä Performance Score: {performance_score}/100")
            if alerts:
                print("üö® Alerts:")
                for alert in alerts:
                    print(f"   {alert}")
            
            # Output results
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"score={performance_score}\n")
                f.write(f"alerts={json.dumps(alerts)}\n")
            
        except Exception as e:
            print(f"üí• Performance monitoring error: {e}")
            alerts.append(f"üí• Monitoring system error: {str(e)}")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write("score=0\n")
                f.write(f"alerts={json.dumps(alerts)}\n")
        EOF

  # DATA DRIFT DETECTION
  data-drift:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type == 'drift' || github.event.inputs.check_type == 'all' || github.event_name == 'schedule'
    outputs:
      drift-detected: ${{ steps.drift.outputs.detected }}
      drift-score: ${{ steps.drift.outputs.score }}
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
    
    - name: üêç Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: üì¶ Install drift detection dependencies
      run: |
        pip install pandas numpy scipy scikit-learn requests mlflow
    
    - name: üîç Detect data drift
      id: drift
      run: |
        python << 'EOF'
        import pandas as pd
        import numpy as np
        from scipy import stats
        import json
        import os
        from datetime import datetime
        
        print("üîç Starting data drift detection...")
        
        # This is a simplified drift detection
        # In production, you'd compare incoming data with training data
        
        # Simulate drift detection (replace with actual data comparison)
        np.random.seed(42)
        
        # Simulate reference data (training data statistics)
        reference_means = np.random.normal(0, 1, 20)
        reference_stds = np.random.uniform(0.5, 2.0, 20)
        
        # Simulate current data statistics
        current_means = reference_means + np.random.normal(0, 0.1, 20)  # Small drift
        current_stds = reference_stds + np.random.normal(0, 0.05, 20)
        
        # Drift detection using KS test simulation
        drift_scores = []
        drift_threshold = 0.05  # p-value threshold
        
        for i in range(20):
            # Simulate KS test p-value
            p_value = abs(current_means[i] - reference_means[i]) / reference_stds[i]
            drift_scores.append(p_value)
        
        # Overall drift assessment
        significant_drifts = sum(1 for score in drift_scores if score > drift_threshold)
        drift_percentage = (significant_drifts / len(drift_scores)) * 100
        
        drift_detected = drift_percentage > 20  # Alert if >20% of features show drift
        overall_drift_score = np.mean(drift_scores)
        
        print(f"üìä Drift Analysis Results:")
        print(f"   Features with significant drift: {significant_drifts}/20 ({drift_percentage:.1f}%)")
        print(f"   Overall drift score: {overall_drift_score:.4f}")
        print(f"   Drift detected: {'‚ö†Ô∏è YES' if drift_detected else '‚úÖ NO'}")
        
        # Output results
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"detected={str(drift_detected).lower()}\n")
            f.write(f"score={overall_drift_score:.4f}\n")
        EOF

  # PROMETHEUS METRICS SETUP
  setup-prometheus:
    runs-on: ubuntu-latest
    needs: [infrastructure-health, model-performance]
    if: always()
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
    
    - name: ‚öôÔ∏è Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: üîß Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: ‚öôÔ∏è Configure kubectl
      run: |
        aws eks update-kubeconfig --region $AWS_REGION --name loan-eks-simple
    
    - name: üìä Deploy Prometheus monitoring stack
      run: |
        # Create monitoring namespace
        kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
        
        # Create Prometheus ConfigMap
        cat << 'EOL' | kubectl apply -f -
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: prometheus-config
          namespace: monitoring
        data:
          prometheus.yml: |
            global:
              scrape_interval: 15s
              evaluation_interval: 15s
            
            rule_files:
              - "ml_alerts.yml"
            
            scrape_configs:
              - job_name: 'kubernetes-pods'
                kubernetes_sd_configs:
                - role: pod
                relabel_configs:
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                  action: replace
                  target_label: __metrics_path__
                  regex: (.+)
                - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                  action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  target_label: __address__
              
              - job_name: 'loan-default-api'
                static_configs:
                  - targets: ['loan-default-api-service.loan-default:8001']
                metrics_path: '/metrics'
                scrape_interval: 30s
        
          ml_alerts.yml: |
            groups:
            - name: ml_model_alerts
              rules:
              - alert: ModelLatencyHigh
                expr: histogram_quantile(0.95, rate(prediction_duration_seconds_bucket[5m])) > 1
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: "Model prediction latency is high"
                  description: "95th percentile latency is {{ $value }}s"
              
              - alert: ModelAccuracyLow
                expr: model_accuracy < 0.7
                for: 5m
                labels:
                  severity: critical
                annotations:
                  summary: "Model accuracy below threshold"
                  description: "Model accuracy is {{ $value }}"
              
              - alert: PodNotReady
                expr: kube_pod_status_ready{condition="false", namespace="loan-default"} > 0
                for: 1m
                labels:
                  severity: warning
                annotations:
                  summary: "Pod not ready"
                  description: "Pod {{ $labels.pod }} is not ready"
        EOL
        
        # Deploy Prometheus
        cat << 'EOL' | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: prometheus
          namespace: monitoring
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: prometheus
          template:
            metadata:
              labels:
                app: prometheus
            spec:
              containers:
              - name: prometheus
                image: prom/prometheus:latest
                ports:
                - containerPort: 9090
                volumeMounts:
                - name: config
                  mountPath: /etc/prometheus
                args:
                  - '--config.file=/etc/prometheus/prometheus.yml'
                  - '--storage.tsdb.path=/prometheus'
                  - '--web.console.libraries=/etc/prometheus/console_libraries'
                  - '--web.console.templates=/etc/prometheus/consoles'
                  - '--web.enable-lifecycle'
              volumes:
              - name: config
                configMap:
                  name: prometheus-config
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: prometheus
          namespace: monitoring
        spec:
          selector:
            app: prometheus
          ports:
            - port: 9090
              targetPort: 9090
          type: LoadBalancer
        EOL
        
        echo "‚úÖ Prometheus deployed successfully"

  # GRAFANA SETUP
  setup-grafana:
    runs-on: ubuntu-latest
    needs: setup-prometheus
    if: always()
    
    steps:
    - name: ‚öôÔ∏è Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: üîß Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: ‚öôÔ∏è Configure kubectl
      run: |
        aws eks update-kubeconfig --region $AWS_REGION --name loan-eks-simple
    
    - name: üìà Deploy Grafana
      run: |
        # Deploy Grafana
        cat << 'EOL' | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: grafana
          namespace: monitoring
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: grafana
          template:
            metadata:
              labels:
                app: grafana
            spec:
              containers:
              - name: grafana
                image: grafana/grafana:latest
                ports:
                - containerPort: 3000
                env:
                - name: GF_SECURITY_ADMIN_PASSWORD
                  value: "admin123"  # Change this in production
                volumeMounts:
                - name: grafana-storage
                  mountPath: /var/lib/grafana
              volumes:
              - name: grafana-storage
                emptyDir: {}
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: grafana
          namespace: monitoring
        spec:
          selector:
            app: grafana
          ports:
            - port: 3000
              targetPort: 3000
          type: LoadBalancer
        EOL
        
        echo "‚úÖ Grafana deployed successfully"
        echo "üìä Default login: admin/admin123"

  # MONITORING REPORT
  monitoring-report:
    runs-on: ubuntu-latest
    needs: [infrastructure-health, model-performance, data-drift, setup-prometheus, setup-grafana]
    if: always()
    
    steps:
    - name: üìä Generate monitoring report
      run: |
        echo "üîç ML System Monitoring Report - $(date)"
        echo "================================================"
        
        echo "üèóÔ∏è Infrastructure Health:"
        echo "   Status: ${{ needs.infrastructure-health.outputs.cluster-healthy == 'true' && '‚úÖ Healthy' || '‚ùå Unhealthy' }}"
        
        echo "üìä Model Performance:"
        echo "   Score: ${{ needs.model-performance.outputs.performance-score }}/100"
        echo "   Alerts: ${{ needs.model-performance.outputs.alerts }}"
        
        echo "üîç Data Drift:"
        echo "   Detected: ${{ needs.data-drift.outputs.drift-detected == 'true' && '‚ö†Ô∏è YES' || '‚úÖ NO' }}"
        echo "   Score: ${{ needs.data-drift.outputs.drift-score }}"
        
        echo "üìà Monitoring Stack:"
        echo "   Prometheus: ${{ needs.setup-prometheus.result }}"
        echo "   Grafana: ${{ needs.setup-grafana.result }}"
        
        echo ""
        echo "üîó Access URLs:"
        echo "   Get LoadBalancer URLs with:"
        echo "   kubectl get svc -n monitoring"
    
    - name: üì¢ Send monitoring alerts
      if: needs.infrastructure-health.outputs.cluster-healthy == 'false' || needs.model-performance.outputs.performance-score < 70 || needs.data-drift.outputs.drift-detected == 'true'
      run: |
        echo "üö® MONITORING ALERT TRIGGERED!"
        echo "One or more monitoring checks failed:"
        echo "- Infrastructure: ${{ needs.infrastructure-health.outputs.cluster-healthy }}"
        echo "- Performance Score: ${{ needs.model-performance.outputs.performance-score }}"
        echo "- Data Drift: ${{ needs.data-drift.outputs.drift-detected }}"
        echo ""
        echo "üîß Recommended Actions:"
        echo "1. Check pod logs: kubectl logs -l app=loan-default-api -n loan-default"
        echo "2. Check resource usage: kubectl top pods -n loan-default"
        echo "3. Review model metrics in MLflow"
        echo "4. Consider retraining if drift detected"