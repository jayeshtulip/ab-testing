name: ğŸš€ Training Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'scripts/**'
      - 'params.yaml'
      - 'dvc.yaml'
      - 'data/raw/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - 'params.yaml'
      - 'dvc.yaml'
  workflow_dispatch:
    inputs:
      experiment_name:
        description: 'Experiment name for this training run'
        required: true
        default: 'manual-training'
      model_params:
        description: 'Model parameters (JSON format)'
        required: false
        default: '{"n_estimators": 100, "max_depth": 10}'

env:
  PYTHON_VERSION: '3.10'
  MLFLOW_TRACKING_URI: http://ab124afa4840a4f8298398f9c7fd7c7e-306571921.ap-south-1.elb.amazonaws.com
  DVC_REMOTE: s3-storage

jobs:
  training:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: ğŸ”§ Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-south-1
    
    - name: ğŸ”§ Configure DVC
      run: |
        dvc remote modify ${{ env.DVC_REMOTE }} region ap-south-1
        dvc remote modify ${{ env.DVC_REMOTE }} use_ssl true
    
    - name: ğŸ“Š Pull data from DVC
      run: |
        dvc pull
        echo "âœ… Data pulled successfully"
        ls -la data/raw/
    
    - name: ğŸ§ª Run data quality checks
      run: |
        echo "ğŸ” Running data quality checks..."
        python scripts/data_check_script.py
        
        # Check if required files exist
        if [ ! -f "data/raw/X.csv" ] || [ ! -f "data/raw/y.csv" ]; then
          echo "âŒ Required data files not found"
          exit 1
        fi
        
        # Basic data validation
        python -c "
        import pandas as pd
        X = pd.read_csv('data/raw/X.csv')
        y = pd.read_csv('data/raw/y.csv')
        assert X.shape[0] > 0, 'X dataset is empty'
        assert y.shape[0] > 0, 'y dataset is empty'
        assert X.shape[0] == y.shape[0], 'X and y have different number of samples'
        print(f'âœ… Data validation passed: {X.shape[0]} samples, {X.shape[1]} features')
        "
    
    - name: ğŸ”§ Update parameters (if provided)
      if: github.event.inputs.model_params != ''
      run: |
        echo "ğŸ”§ Updating model parameters..."
        python -c "
        import yaml
        import json
        
        # Load existing params
        with open('params.yaml', 'r') as f:
            params = yaml.safe_load(f)
        
        # Update with new params
        new_params = json.loads('${{ github.event.inputs.model_params }}')
        params['model_training'].update(new_params)
        
        # Save updated params
        with open('params.yaml', 'w') as f:
            yaml.dump(params, f, default_flow_style=False, indent=2)
        
        print('âœ… Parameters updated')
        print(yaml.dump(params, default_flow_style=False, indent=2))
        "
    
    - name: ğŸš€ Run DVC pipeline
      run: |
        echo "ğŸš€ Starting DVC pipeline..."
        
        # Run the complete pipeline
        dvc repro --force
        
        # Check if pipeline completed successfully
        if [ $? -eq 0 ]; then
          echo "âœ… DVC pipeline completed successfully"
        else
          echo "âŒ DVC pipeline failed"
          exit 1
        fi
    
    - name: ğŸ“Š Extract metrics
      id: metrics
      run: |
        echo "ğŸ“Š Extracting model metrics..."
        
        # Extract metrics from training
        if [ -f "metrics/train_metrics.json" ]; then
          ACCURACY=$(python -c "import json; print(json.load(open('metrics/train_metrics.json'))['test_accuracy'])")
          AUC=$(python -c "import json; print(json.load(open('metrics/train_metrics.json'))['auc_score'])")
          F1=$(python -c "import json; print(json.load(open('metrics/train_metrics.json'))['f1_score'])")
          
          echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
          echo "auc=$AUC" >> $GITHUB_OUTPUT
          echo "f1=$F1" >> $GITHUB_OUTPUT
          
          echo "âœ… Metrics extracted:"
          echo "   Accuracy: $ACCURACY"
          echo "   AUC: $AUC"
          echo "   F1: $F1"
        else
          echo "âŒ Training metrics file not found"
          exit 1
        fi
    
    - name: âœ… Validate model performance
      run: |
        echo "âœ… Validating model performance..."
        
        ACCURACY=${{ steps.metrics.outputs.accuracy }}
        AUC=${{ steps.metrics.outputs.auc }}
        
        # Define minimum thresholds
        MIN_ACCURACY=0.70
        MIN_AUC=0.65
        
        echo "ğŸ¯ Performance thresholds:"
        echo "   Minimum Accuracy: $MIN_ACCURACY"
        echo "   Minimum AUC: $MIN_AUC"
        echo "   Actual Accuracy: $ACCURACY"
        echo "   Actual AUC: $AUC"
        
        # Validate accuracy
        python -c "
        accuracy = float('$ACCURACY')
        min_accuracy = float('$MIN_ACCURACY')
        assert accuracy >= min_accuracy, f'Accuracy {accuracy} below threshold {min_accuracy}'
        print(f'âœ… Accuracy validation passed: {accuracy} >= {min_accuracy}')
        "
        
        # Validate AUC
        python -c "
        auc = float('$AUC')
        min_auc = float('$MIN_AUC')
        assert auc >= min_auc, f'AUC {auc} below threshold {min_auc}'
        print(f'âœ… AUC validation passed: {auc} >= {min_auc}')
        "
        
        echo "ğŸ‰ Model performance validation passed!"
    
    - name: ğŸ“¤ Push artifacts to DVC
      run: |
        echo "ğŸ“¤ Pushing artifacts to DVC remote..."
        dvc push
        echo "âœ… Artifacts pushed successfully"
    
    - name: ğŸ·ï¸ Tag successful model
      if: github.ref == 'refs/heads/main'
      run: |
        echo "ğŸ·ï¸ Tagging successful model..."
        
        # Create a unique tag for this model
        TIMESTAMP=$(date +%Y%m%d-%H%M%S)
        COMMIT_SHA=$(git rev-parse --short HEAD)
        ACCURACY=${{ steps.metrics.outputs.accuracy }}
        
        TAG_NAME="model-v${TIMESTAMP}-${COMMIT_SHA}-acc$(echo $ACCURACY | tr '.' '_')"
        
        git tag $TAG_NAME
        echo "âœ… Created tag: $TAG_NAME"
        echo "tag_name=$TAG_NAME" >> $GITHUB_OUTPUT
    
    - name: ğŸ“‹ Generate training report
      run: |
        echo "ğŸ“‹ Generating training report..."
        
        cat > training_report.md << EOF
        # Training Report
        
        ## Experiment Details
        - **Timestamp**: $(date)
        - **Commit**: ${{ github.sha }}
        - **Branch**: ${{ github.ref_name }}
        - **Experiment Name**: ${{ github.event.inputs.experiment_name || 'automated' }}
        
        ## Model Performance
        - **Test Accuracy**: ${{ steps.metrics.outputs.accuracy }}
        - **AUC Score**: ${{ steps.metrics.outputs.auc }}
        - **F1 Score**: ${{ steps.metrics.outputs.f1 }}
        
        ## Pipeline Status
        - âœ… Data validation passed
        - âœ… Model training completed
        - âœ… Performance validation passed
        - âœ… Artifacts pushed to remote
        
        ## Next Steps
        - Model ready for staging deployment
        - Consider running evaluation tests
        - Review feature importance plots
        EOF
        
        echo "âœ… Training report generated"
    
    - name: ğŸ“¤ Upload artifacts
      uses: actions/upload-artifact@v3
      with:
        name: training-artifacts
        path: |
          models/
          metrics/
          plots/
          training_report.md
        retention-days: 30
    
    - name: ğŸ’¬ Comment on PR (if PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const accuracy = '${{ steps.metrics.outputs.accuracy }}';
          const auc = '${{ steps.metrics.outputs.auc }}';
          const f1 = '${{ steps.metrics.outputs.f1 }}';
          
          const comment = `## ğŸ¤– Training Pipeline Results
          
          **Model Performance:**
          - **Accuracy**: ${accuracy}
          - **AUC Score**: ${auc}
          - **F1 Score**: ${f1}
          
          **Status**: âœ… All validations passed
          
          The model is ready for review and potential deployment.`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  notify:
    needs: training
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: ğŸ“§ Notify on success
      if: needs.training.result == 'success'
      run: |
        echo "ğŸ‰ Training pipeline completed successfully!"
        echo "âœ… Model performance meets requirements"
        echo "ğŸ“Š Accuracy: ${{ needs.training.outputs.accuracy }}"
        
    - name: ğŸš¨ Notify on failure
      if: needs.training.result == 'failure'
      run: |
        echo "âŒ Training pipeline failed!"
        echo "ğŸ” Please check the logs for details"
        # In production, you would send notifications via Slack, email, etc.