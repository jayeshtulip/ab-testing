name: Training Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'scripts/**'
      - 'params.yaml'
      - 'dvc.yaml'
      - 'data/raw/**'
      - '.github/workflows/training-pipeline.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - 'params.yaml'
      - 'dvc.yaml'
  workflow_dispatch:
    inputs:
      experiment_name:
        description: 'Experiment name for this training run'
        required: true
        default: 'manual-training'
      force_retrain:
        description: 'Force complete retraining (ignore cache)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.10'
  MLFLOW_TRACKING_URI: http://ab124afa4840a4f8298398f9c7fd7c7e-306571921.ap-south-1.elb.amazonaws.com
  AWS_REGION: ap-south-1

jobs:
  # DATA VALIDATION AND PREPARATION
  validate-data:
    runs-on: ubuntu-latest
    outputs:
      data-changed: ${{ steps.check-data.outputs.changed }}
      data-hash: ${{ steps.check-data.outputs.hash }}
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies with S3 support
      run: |
        python -m pip install --upgrade pip setuptools wheel
        
        # Install system dependencies that DVC might need
        sudo apt-get update
        sudo apt-get install -y git
        
        # Install core dependencies first
        pip install --no-cache-dir boto3==1.39.11 botocore s3fs
        
        # Install DVC without extras first, then add S3 support
        pip install --no-cache-dir dvc==3.51.2
        pip install --no-cache-dir dvc-s3
        
        # Install ML dependencies
        pip install mlflow psycopg2-binary pandas scikit-learn numpy pyyaml
        
        # Verify installations step by step
        python -c "import dvc; print(f'âœ… DVC: {dvc.__version__}')"
        python -c "import boto3; print(f'âœ… Boto3: {boto3.__version__}')"
        
        # Test DVC CLI instead of importing modules directly
        dvc version
        echo "âœ… DVC CLI working"
        
        # Don't test dvc.remote import - just ensure DVC CLI works
        python -c "
        import subprocess
        result = subprocess.run(['dvc', '--help'], capture_output=True, text=True)
        if result.returncode == 0:
            print('âœ… DVC command line interface working')
        else:
            print('âŒ DVC CLI not working')
            exit(1)
        "
    
    - name: âš™ï¸ Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: ğŸ”§ Setup DVC
      run: |
        dvc remote modify s3-storage region $AWS_REGION
        dvc remote default s3-storage
        echo "âœ… DVC configured for region: $AWS_REGION"
        echo "ğŸ“ Default remote: $(dvc remote default)"
        dvc remote list
    
    - name: ğŸ“Š Pull and validate data
      id: check-data
      run: |
        echo "ğŸ“¥ Pulling latest data from DVC remote..."
        dvc pull --verbose --force
        
        # Check if data has changed
        DATA_HASH=$(find data/raw -name "*.csv" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
        echo "ğŸ“Š Current data hash: $DATA_HASH"
        
        # Store hash for comparison
        echo "hash=$DATA_HASH" >> $GITHUB_OUTPUT
        
        # Check if we should retrain
        force_retrain="${{ github.event.inputs.force_retrain }}"
        if [[ "$force_retrain" == "true" ]]; then
          echo "ğŸ”„ Force retrain enabled"
          echo "changed=true" >> $GITHUB_OUTPUT
        else
          echo "ğŸ“ˆ Data validation passed"
          echo "changed=true" >> $GITHUB_OUTPUT  # Always true for now, add logic later
        fi
        
        # Basic data validation
        python << 'PYTHON'
        import pandas as pd
        import os
        
        try:
            # Debug: Show current directory and contents
            print(f"ğŸ” Current working directory: {os.getcwd()}")
            print("ğŸ“ Directory contents:")
            for root, dirs, files in os.walk('.'):
                if 'data' in root or any('.csv' in f for f in files):
                    print(f"  {root}: {files}")
            
            # Check if data files exist and are valid
            data_raw_path = 'data/raw'
            if os.path.exists(data_raw_path):
                print(f"âœ… Found data/raw directory")
                csv_files = [f for f in os.listdir(data_raw_path) if f.endswith('.csv')]
                print(f"ğŸ“Š CSV files in data/raw: {csv_files}")
            else:
                print(f"âŒ data/raw directory not found")
                # Try to find CSV files anywhere
                import glob
                csv_files = glob.glob('**/*.csv', recursive=True)
                print(f"ğŸ” CSV files found anywhere: {csv_files}")
            
            if os.path.exists('data/raw/X.csv'):
                X = pd.read_csv('data/raw/X.csv')
                print(f"âœ… Features data: {X.shape}")
                
            if os.path.exists('data/raw/y.csv'):
                y = pd.read_csv('data/raw/y.csv')
                print(f"âœ… Target data: {y.shape}")
                
            print("âœ… Data validation passed")
            
        except Exception as e:
            print(f"âŒ Data validation failed: {e}")
            print("ğŸ” Available files:")
            import glob
            all_files = glob.glob('**/*', recursive=True)
            relevant_files = [f for f in all_files if any(ext in f for ext in ['.csv', '.dvc', 'data'])]
            for f in relevant_files[:20]:  # Show first 20 relevant files
                print(f"  {f}")
            exit(1)
        PYTHON

  # MODEL TRAINING WITH MLFLOW TRACKING
  training:
    runs-on: ubuntu-latest
    needs: validate-data
    if: needs.validate-data.outputs.data-changed == 'true'
    timeout-minutes: 30
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies with S3 support
      run: |
        python -m pip install --upgrade pip setuptools wheel
        
        # Install system dependencies
        sudo apt-get update
        sudo apt-get install -y git
        
        # Install AWS and S3 dependencies first
        pip install --no-cache-dir boto3==1.39.11 botocore s3fs
        
        # Install DVC and S3 support separately to avoid conflicts
        pip install --no-cache-dir dvc==3.51.2
        pip install --no-cache-dir dvc-s3
        
        # Install ML dependencies
        pip install mlflow psycopg2-binary pandas scikit-learn numpy pyyaml
        
        # Verify core installations
        python -c "import dvc; print(f'âœ… DVC: {dvc.__version__}')"
        python -c "import boto3; print(f'âœ… Boto3: {boto3.__version__}')"
        
        # Test DVC CLI functionality (more reliable than module imports)
        dvc version
        echo "âœ… DVC CLI working"
        
        # Test that DVC can handle S3 (without importing modules)
        python -c "
        import subprocess
        result = subprocess.run(['dvc', 'remote', '-h'], capture_output=True, text=True)
        if result.returncode == 0:
            print('âœ… DVC remote commands available')
        else:
            print('âŒ DVC remote commands not working')
            exit(1)
        "
    
    - name: âš™ï¸ Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: ğŸ”§ Setup DVC and pull data
      run: |
        dvc remote modify s3-storage region $AWS_REGION
        dvc remote default s3-storage
        dvc remote list
        echo "ğŸ“ Default remote: $(dvc remote default)"
        dvc pull --verbose --force
        
        echo "ğŸ” Debugging data location..."
        echo "ğŸ“ Current directory structure:"
        find . -name "*.csv" -type f | head -10
        echo "ğŸ“‚ Data directory contents:"
        ls -la data/ || echo "âŒ No data directory"
        ls -la data/raw/ || echo "âŒ No data/raw directory"
        echo "ğŸ“‹ DVC tracked files:"
        find . -name "*.dvc" -type f
        echo "âœ… Data pull completed"
    
    - name: ğŸ§ª Test MLflow connection
      run: |
        python << 'EOF'
        import mlflow
        import requests
        import os
        
        mlflow_uri = os.environ['MLFLOW_TRACKING_URI']
        mlflow.set_tracking_uri(mlflow_uri)
        
        # Test connection
        try:
            response = requests.get(f"{mlflow_uri}/health", timeout=10)
            print(f"âœ… MLflow health check: {response.status_code}")
            
            client = mlflow.MlflowClient()
            experiments = client.search_experiments()
            print(f"âœ… MLflow client working - {len(experiments)} experiments found")
            
        except Exception as e:
            print(f"âŒ MLflow connection failed: {e}")
            exit(1)
        EOF
    
    - name: ğŸ¤– Run DVC training pipeline
      env:
        EXPERIMENT_NAME: ${{ github.event.inputs.experiment_name || 'automated-training' }}
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_ACTOR: ${{ github.actor }}
      run: |
        echo "ğŸš€ Starting training pipeline..."
        echo "ğŸ“Š Experiment: $EXPERIMENT_NAME"
        echo "ğŸ“ˆ Data hash: ${{ needs.validate-data.outputs.data-hash }}"
        
        # Check DVC status before running
        dvc status
        
        # Run DVC pipeline with MLflow tracking
        dvc repro --verbose
        
        echo "âœ… DVC pipeline completed"
    
    - name: ğŸ“Š Log training results
      run: |
        echo "ğŸ“ˆ Training pipeline completed successfully!"
        
        # Check if models were created
        if [ -d "models" ]; then
          echo "ğŸ“¦ Models directory contents:"
          ls -la models/
        fi
        
        # Check if metrics were generated
        if [ -d "metrics" ]; then
          echo "ğŸ“Š Metrics directory contents:"
          ls -la metrics/
          
          if [ -f "metrics/train_metrics.json" ]; then
            echo "ğŸ“ˆ Training metrics:"
            cat metrics/train_metrics.json
          fi
        fi
    
    - name: ğŸ” Validate trained model
      run: |
        python << 'EOF'
        import mlflow
        import os
        import json
        
        try:
            mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
            client = mlflow.MlflowClient()
            
            # Get the latest model
            model_name = "loan-default-model"
            latest_versions = client.get_latest_versions(model_name, stages=["None"])
            
            if latest_versions:
                latest_model = latest_versions[0]
                run = client.get_run(latest_model.run_id)
                
                print(f"âœ… Latest model: v{latest_model.version}")
                print(f"ğŸ“Š Model metrics:")
                
                metrics = run.data.metrics
                for metric, value in metrics.items():
                    print(f"   {metric}: {value:.4f}")
                
                # Basic quality check
                accuracy = metrics.get('accuracy', 0)
                if accuracy > 0.7:
                    print(f"âœ… Model quality acceptable: {accuracy:.4f}")
                else:
                    print(f"âš ï¸  Model quality below threshold: {accuracy:.4f}")
                    
            else:
                print("âš ï¸  No models found in registry")
                
        except Exception as e:
            print(f"âš ï¸  Could not validate model: {e}")
        EOF
    
    - name: ğŸš€ Push DVC changes
      run: |
        echo "ğŸ“¤ Pushing DVC artifacts to remote storage..."
        dvc push --verbose --force
        echo "âœ… DVC artifacts pushed successfully"
    
    - name: ğŸ“ Commit DVC changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add DVC files if they changed
        git add dvc.lock metrics/ plots/ -f 2>/dev/null || true
        
        if git diff --staged --quiet; then
          echo "ğŸ“ No DVC changes to commit"
        else
          git commit -m "Update DVC pipeline results - ${{ github.event.inputs.experiment_name || 'automated' }}"
          echo "âœ… DVC changes committed"
        fi

  # MODEL EVALUATION AND COMPARISON
  evaluate-model:
    runs-on: ubuntu-latest
    needs: [validate-data, training]
    if: needs.training.result == 'success'
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        pip install mlflow pandas numpy scikit-learn boto3
    
    - name: ğŸ“Š Evaluate model performance
      run: |
        python << 'EOF'
        import mlflow
        import os
        import json
        
        try:
            mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
            client = mlflow.MlflowClient()
            
            model_name = "loan-default-model"
            
            # Get latest model
            latest_versions = client.get_latest_versions(model_name, stages=["None"])
            
            if not latest_versions:
                print("âŒ No models found for evaluation")
                exit(1)
            
            latest_model = latest_versions[0]
            run = client.get_run(latest_model.run_id)
            metrics = run.data.metrics
            
            print(f"ğŸ“Š Evaluating model v{latest_model.version}")
            print(f"ğŸ” Run ID: {latest_model.run_id}")
            
            # Performance evaluation
            performance_report = {
                'model_version': latest_model.version,
                'run_id': latest_model.run_id,
                'metrics': dict(metrics),
                'evaluation_timestamp': run.info.end_time,
                'experiment_name': "${{ github.event.inputs.experiment_name || 'automated' }}"
            }
            
            # Quality assessment
            accuracy = metrics.get('accuracy', 0)
            f1_score = metrics.get('f1_score', 0)
            
            print(f"ğŸ“ˆ Performance Summary:")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   F1-Score: {f1_score:.4f}")
            
            # Determine recommendation
            if accuracy >= 0.80 and f1_score >= 0.75:
                recommendation = "EXCELLENT - Ready for production"
                quality_level = "excellent"
            elif accuracy >= 0.75 and f1_score >= 0.70:
                recommendation = "GOOD - Ready for staging"
                quality_level = "good"
            elif accuracy >= 0.70 and f1_score >= 0.65:
                recommendation = "ACCEPTABLE - Needs monitoring"
                quality_level = "acceptable"
            else:
                recommendation = "POOR - Requires improvement"
                quality_level = "poor"
            
            performance_report['recommendation'] = recommendation
            performance_report['quality_level'] = quality_level
            
            print(f"ğŸ¯ Recommendation: {recommendation}")
            
            # Save evaluation report
            with open('model_evaluation_report.json', 'w') as f:
                json.dump(performance_report, f, indent=2, default=str)
            
            print("âœ… Model evaluation completed")
            
        except Exception as e:
            print(f"âŒ Model evaluation failed: {e}")
            exit(1)
        EOF
    
    - name: ğŸ“¤ Upload evaluation report
      uses: actions/upload-artifact@v4
      with:
        name: model-evaluation-report
        path: model_evaluation_report.json
        retention-days: 30

  # PROMOTE MODEL IF QUALITY IS GOOD
  promote-model:
    runs-on: ubuntu-latest
    needs: [training, evaluate-model]
    if: needs.evaluate-model.result == 'success'
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: ğŸ“¦ Install MLflow
      run: pip install mlflow boto3
    
    - name: ğŸ† Promote model to staging
      run: |
        python << 'EOF'
        import mlflow
        import os
        
        try:
            mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
            client = mlflow.MlflowClient()
            
            model_name = "loan-default-model"
            
            # Get latest model
            latest_versions = client.get_latest_versions(model_name, stages=["None"])
            
            if not latest_versions:
                print("âŒ No models found for promotion")
                exit(1)
            
            latest_model = latest_versions[0]
            run = client.get_run(latest_model.run_id)
            metrics = run.data.metrics
            
            accuracy = metrics.get('accuracy', 0)
            f1_score = metrics.get('f1_score', 0)
            
            print(f"ğŸ” Evaluating model v{latest_model.version} for promotion")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   F1-Score: {f1_score:.4f}")
            
            # Promotion logic based on quality
            if accuracy >= 0.75 and f1_score >= 0.70:
                # Promote to Staging
                client.transition_model_version_stage(
                    name=model_name,
                    version=latest_model.version,
                    stage="Staging",
                    archive_existing_versions=True
                )
                print(f"âœ… Model v{latest_model.version} promoted to Staging")
                
                # If really good, consider for production
                if accuracy >= 0.80 and f1_score >= 0.75:
                    print(f"ğŸ¯ Model quality excellent - candidate for production promotion")
                    
            else:
                print(f"âš ï¸  Model quality below promotion threshold")
                print(f"   Required: Accuracy â‰¥ 0.75, F1 â‰¥ 0.70")
                print(f"   Actual:   Accuracy = {accuracy:.4f}, F1 = {f1_score:.4f}")
                
        except Exception as e:
            print(f"âŒ Model promotion failed: {e}")
        EOF

  # TRIGGER DEPLOYMENT PIPELINE
  trigger-deployment:
    runs-on: ubuntu-latest
    needs: [training, evaluate-model, promote-model]
    if: needs.promote-model.result == 'success'
    
    steps:
    - name: ğŸš€ Trigger deployment pipeline
      run: |
        echo "ğŸš€ Training pipeline completed successfully!"
        echo "ğŸ“Š Model has been trained and evaluated"
        echo "ğŸ¯ Deployment pipeline will be triggered automatically"
        echo ""
        echo "ğŸ“‹ Training Summary:"
        echo "   âœ… Data validation passed"
        echo "   âœ… Model training completed"  
        echo "   âœ… Model evaluation completed"
        echo "   âœ… Model promoted to staging"
        echo ""
        echo "â­ï¸  Next: Model Deployment pipeline will validate and deploy the model"

  # NOTIFICATION
  notify:
    runs-on: ubuntu-latest
    needs: [validate-data, training, evaluate-model, promote-model, trigger-deployment]
    if: always()
    
    steps:
    - name: ğŸ“Š Determine training status
      id: status
      run: |
        if [[ "${{ needs.training.result }}" == "success" && "${{ needs.evaluate-model.result }}" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "message=âœ… Training pipeline completed successfully" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.validate-data.result }}" == "failure" ]]; then
          echo "status=data_failed" >> $GITHUB_OUTPUT
          echo "message=âŒ Data validation failed" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.training.result }}" == "failure" ]]; then
          echo "status=training_failed" >> $GITHUB_OUTPUT
          echo "message=âŒ Model training failed" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.evaluate-model.result }}" == "failure" ]]; then
          echo "status=evaluation_failed" >> $GITHUB_OUTPUT
          echo "message=âŒ Model evaluation failed" >> $GITHUB_OUTPUT
        else
          echo "status=unknown" >> $GITHUB_OUTPUT
          echo "message=â“ Training pipeline completed with unknown status" >> $GITHUB_OUTPUT
        fi
    
    - name: ğŸ“¢ Send training notification
      run: |
        echo "ğŸ“¢ Training Pipeline Status: ${{ steps.status.outputs.message }}"
        echo "ğŸ§ª Experiment: ${{ github.event.inputs.experiment_name || 'automated-training' }}"
        echo "ğŸ“Š Data Hash: ${{ needs.validate-data.outputs.data-hash }}"
        echo "ğŸ‘¤ Triggered by: ${{ github.actor }}"
        echo "ğŸ”— Workflow: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        echo ""
        if [[ "${{ steps.status.outputs.status }}" == "success" ]]; then
          echo "â­ï¸  Next: Deployment pipeline will be triggered automatically"
        fi