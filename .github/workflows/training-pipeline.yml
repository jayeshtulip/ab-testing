name: Training Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'scripts/**'
      - 'params.yaml'
      - 'dvc.yaml'
      - 'data/raw/**'
      - '.github/workflows/training-pipeline.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - 'params.yaml'
      - 'dvc.yaml'
  workflow_dispatch:
    inputs:
      experiment_name:
        description: 'Experiment name for this training run'
        required: true
        default: 'manual-training'
      force_retrain:
        description: 'Force complete retraining (ignore cache)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.10'
  MLFLOW_TRACKING_URI: http://ab124afa4840a4f8298398f9c7fd7c7e-306571921.ap-south-1.elb.amazonaws.com
  AWS_REGION: ap-south-1

jobs:
  # DATA VALIDATION AND PREPARATION
  validate-data:
    runs-on: ubuntu-latest
    outputs:
      data-changed: ${{ steps.check-data.outputs.changed }}
      data-hash: ${{ steps.check-data.outputs.hash }}
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies with S3 support
      run: |
        python -m pip install --upgrade pip setuptools wheel
        
        # Install system dependencies that DVC might need
        sudo apt-get update
        sudo apt-get install -y git
        
        # Install core dependencies first
        pip install --no-cache-dir boto3==1.39.11 botocore s3fs
        
        # Install DVC without extras first, then add S3 support
        pip install --no-cache-dir dvc==3.51.2
        pip install --no-cache-dir dvc-s3
        
        # Install ML dependencies
        pip install mlflow psycopg2-binary pandas scikit-learn numpy pyyaml
        
        # Verify installations step by step
        python -c "import dvc; print(f'✅ DVC: {dvc.__version__}')"
        python -c "import boto3; print(f'✅ Boto3: {boto3.__version__}')"
        
        # Test DVC CLI instead of importing modules directly
        dvc version
        echo "✅ DVC CLI working"
        
        # Don't test dvc.remote import - just ensure DVC CLI works
        python -c "
        import subprocess
        result = subprocess.run(['dvc', '--help'], capture_output=True, text=True)
        if result.returncode == 0:
            print('✅ DVC command line interface working')
        else:
            print('❌ DVC CLI not working')
            exit(1)
        "
    
    - name: ⚙️ Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: 🔧 Setup DVC
      run: |
        dvc remote modify s3-storage region $AWS_REGION
        dvc remote default s3-storage
        echo "✅ DVC configured for region: $AWS_REGION"
        echo "📍 Default remote: $(dvc remote default)"
        dvc remote list
    
    - name: 📊 Pull and validate data
      id: check-data
      run: |
        echo "📥 Pulling latest data from DVC remote..."
        dvc pull --verbose --force
        
        # Check if data has changed
        DATA_HASH=$(find data/raw -name "*.csv" -exec md5sum {} \; | sort | md5sum | cut -d' ' -f1)
        echo "📊 Current data hash: $DATA_HASH"
        
        # Store hash for comparison
        echo "hash=$DATA_HASH" >> $GITHUB_OUTPUT
        
        # Check if we should retrain
        force_retrain="${{ github.event.inputs.force_retrain }}"
        if [[ "$force_retrain" == "true" ]]; then
          echo "🔄 Force retrain enabled"
          echo "changed=true" >> $GITHUB_OUTPUT
        else
          echo "📈 Data validation passed"
          echo "changed=true" >> $GITHUB_OUTPUT  # Always true for now, add logic later
        fi
        
        # Basic data validation
        python << 'PYTHON'
        import pandas as pd
        import os
        
        try:
            # Debug: Show current directory and contents
            print(f"🔍 Current working directory: {os.getcwd()}")
            print("📁 Directory contents:")
            for root, dirs, files in os.walk('.'):
                if 'data' in root or any('.csv' in f for f in files):
                    print(f"  {root}: {files}")
            
            # Check if data files exist and are valid
            data_raw_path = 'data/raw'
            if os.path.exists(data_raw_path):
                print(f"✅ Found data/raw directory")
                csv_files = [f for f in os.listdir(data_raw_path) if f.endswith('.csv')]
                print(f"📊 CSV files in data/raw: {csv_files}")
            else:
                print(f"❌ data/raw directory not found")
                # Try to find CSV files anywhere
                import glob
                csv_files = glob.glob('**/*.csv', recursive=True)
                print(f"🔍 CSV files found anywhere: {csv_files}")
            
            if os.path.exists('data/raw/X.csv'):
                X = pd.read_csv('data/raw/X.csv')
                print(f"✅ Features data: {X.shape}")
                
            if os.path.exists('data/raw/y.csv'):
                y = pd.read_csv('data/raw/y.csv')
                print(f"✅ Target data: {y.shape}")
                
            print("✅ Data validation passed")
            
        except Exception as e:
            print(f"❌ Data validation failed: {e}")
            print("🔍 Available files:")
            import glob
            all_files = glob.glob('**/*', recursive=True)
            relevant_files = [f for f in all_files if any(ext in f for ext in ['.csv', '.dvc', 'data'])]
            for f in relevant_files[:20]:  # Show first 20 relevant files
                print(f"  {f}")
            exit(1)
        PYTHON

  # MODEL TRAINING WITH MLFLOW TRACKING
  training:
    runs-on: ubuntu-latest
    needs: validate-data
    if: needs.validate-data.outputs.data-changed == 'true'
    timeout-minutes: 30
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies with S3 support
      run: |
        python -m pip install --upgrade pip setuptools wheel
        
        # Install system dependencies
        sudo apt-get update
        sudo apt-get install -y git
        
        # Install AWS and S3 dependencies first
        pip install --no-cache-dir boto3==1.39.11 botocore s3fs
        
        # Install DVC and S3 support separately to avoid conflicts
        pip install --no-cache-dir dvc==3.51.2
        pip install --no-cache-dir dvc-s3
        
        # Install ML dependencies
        pip install mlflow psycopg2-binary pandas scikit-learn numpy pyyaml
        
        # Verify core installations
        python -c "import dvc; print(f'✅ DVC: {dvc.__version__}')"
        python -c "import boto3; print(f'✅ Boto3: {boto3.__version__}')"
        
        # Test DVC CLI functionality (more reliable than module imports)
        dvc version
        echo "✅ DVC CLI working"
        
        # Test that DVC can handle S3 (without importing modules)
        python -c "
        import subprocess
        result = subprocess.run(['dvc', 'remote', '-h'], capture_output=True, text=True)
        if result.returncode == 0:
            print('✅ DVC remote commands available')
        else:
            print('❌ DVC remote commands not working')
            exit(1)
        "
    
    - name: ⚙️ Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: 🔧 Setup DVC and pull data
      run: |
        dvc remote modify s3-storage region $AWS_REGION
        dvc remote default s3-storage
        dvc remote list
        echo "📍 Default remote: $(dvc remote default)"
        dvc pull --verbose --force
        
        echo "🔍 Debugging data location..."
        echo "📁 Current directory structure:"
        find . -name "*.csv" -type f | head -10
        echo "📂 Data directory contents:"
        ls -la data/ || echo "❌ No data directory"
        ls -la data/raw/ || echo "❌ No data/raw directory"
        echo "📋 DVC tracked files:"
        find . -name "*.dvc" -type f
        echo "✅ Data pull completed"
    
    - name: 🧪 Test MLflow connection
      run: |
        python << 'EOF'
        import mlflow
        import requests
        import os
        
        mlflow_uri = os.environ['MLFLOW_TRACKING_URI']
        mlflow.set_tracking_uri(mlflow_uri)
        
        # Test connection
        try:
            response = requests.get(f"{mlflow_uri}/health", timeout=10)
            print(f"✅ MLflow health check: {response.status_code}")
            
            client = mlflow.MlflowClient()
            experiments = client.search_experiments()
            print(f"✅ MLflow client working - {len(experiments)} experiments found")
            
        except Exception as e:
            print(f"❌ MLflow connection failed: {e}")
            exit(1)
        EOF
    
    - name: 🤖 Run DVC training pipeline
      env:
        EXPERIMENT_NAME: ${{ github.event.inputs.experiment_name || 'automated-training' }}
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_ACTOR: ${{ github.actor }}
      run: |
        echo "🚀 Starting training pipeline..."
        echo "📊 Experiment: $EXPERIMENT_NAME"
        echo "📈 Data hash: ${{ needs.validate-data.outputs.data-hash }}"
        
        # Check DVC status before running
        dvc status
        
        # Run DVC pipeline with MLflow tracking
        dvc repro --verbose
        
        echo "✅ DVC pipeline completed"
    
    - name: 📊 Log training results
      run: |
        echo "📈 Training pipeline completed successfully!"
        
        # Check if models were created
        if [ -d "models" ]; then
          echo "📦 Models directory contents:"
          ls -la models/
        fi
        
        # Check if metrics were generated
        if [ -d "metrics" ]; then
          echo "📊 Metrics directory contents:"
          ls -la metrics/
          
          if [ -f "metrics/train_metrics.json" ]; then
            echo "📈 Training metrics:"
            cat metrics/train_metrics.json
          fi
        fi
    
    - name: 🔍 Validate trained model
      run: |
        python << 'EOF'
        import mlflow
        import os
        import json
        
        try:
            mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
            client = mlflow.MlflowClient()
            
            # Get the latest model
            model_name = "loan-default-model"
            latest_versions = client.get_latest_versions(model_name, stages=["None"])
            
            if latest_versions:
                latest_model = latest_versions[0]
                run = client.get_run(latest_model.run_id)
                
                print(f"✅ Latest model: v{latest_model.version}")
                print(f"📊 Model metrics:")
                
                metrics = run.data.metrics
                for metric, value in metrics.items():
                    print(f"   {metric}: {value:.4f}")
                
                # Basic quality check
                accuracy = metrics.get('accuracy', 0)
                if accuracy > 0.7:
                    print(f"✅ Model quality acceptable: {accuracy:.4f}")
                else:
                    print(f"⚠️  Model quality below threshold: {accuracy:.4f}")
                    
            else:
                print("⚠️  No models found in registry")
                
        except Exception as e:
            print(f"⚠️  Could not validate model: {e}")
        EOF
    
    - name: 🚀 Push DVC changes
      run: |
        echo "📤 Pushing DVC artifacts to remote storage..."
        dvc push --verbose --force
        echo "✅ DVC artifacts pushed successfully"
    
    - name: 📝 Commit DVC changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add DVC files if they changed
        git add dvc.lock metrics/ plots/ -f 2>/dev/null || true
        
        if git diff --staged --quiet; then
          echo "📝 No DVC changes to commit"
        else
          git commit -m "Update DVC pipeline results - ${{ github.event.inputs.experiment_name || 'automated' }}"
          echo "✅ DVC changes committed"
        fi

  # MODEL EVALUATION AND COMPARISON
  evaluate-model:
    runs-on: ubuntu-latest
    needs: [validate-data, training]
    if: needs.training.result == 'success'
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies
      run: |
        pip install mlflow pandas numpy scikit-learn boto3
    
    - name: 📊 Evaluate model performance
      run: |
        python << 'EOF'
        import mlflow
        import os
        import json
        
        try:
            mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
            client = mlflow.MlflowClient()
            
            model_name = "loan-default-model"
            
            # Get latest model
            latest_versions = client.get_latest_versions(model_name, stages=["None"])
            
            if not latest_versions:
                print("❌ No models found for evaluation")
                exit(1)
            
            latest_model = latest_versions[0]
            run = client.get_run(latest_model.run_id)
            metrics = run.data.metrics
            
            print(f"📊 Evaluating model v{latest_model.version}")
            print(f"🔍 Run ID: {latest_model.run_id}")
            
            # Performance evaluation
            performance_report = {
                'model_version': latest_model.version,
                'run_id': latest_model.run_id,
                'metrics': dict(metrics),
                'evaluation_timestamp': run.info.end_time,
                'experiment_name': "${{ github.event.inputs.experiment_name || 'automated' }}"
            }
            
            # Quality assessment
            accuracy = metrics.get('accuracy', 0)
            f1_score = metrics.get('f1_score', 0)
            
            print(f"📈 Performance Summary:")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   F1-Score: {f1_score:.4f}")
            
            # Determine recommendation
            if accuracy >= 0.80 and f1_score >= 0.75:
                recommendation = "EXCELLENT - Ready for production"
                quality_level = "excellent"
            elif accuracy >= 0.75 and f1_score >= 0.70:
                recommendation = "GOOD - Ready for staging"
                quality_level = "good"
            elif accuracy >= 0.70 and f1_score >= 0.65:
                recommendation = "ACCEPTABLE - Needs monitoring"
                quality_level = "acceptable"
            else:
                recommendation = "POOR - Requires improvement"
                quality_level = "poor"
            
            performance_report['recommendation'] = recommendation
            performance_report['quality_level'] = quality_level
            
            print(f"🎯 Recommendation: {recommendation}")
            
            # Save evaluation report
            with open('model_evaluation_report.json', 'w') as f:
                json.dump(performance_report, f, indent=2, default=str)
            
            print("✅ Model evaluation completed")
            
        except Exception as e:
            print(f"❌ Model evaluation failed: {e}")
            exit(1)
        EOF
    
    - name: 📤 Upload evaluation report
      uses: actions/upload-artifact@v4
      with:
        name: model-evaluation-report
        path: model_evaluation_report.json
        retention-days: 30

  # PROMOTE MODEL IF QUALITY IS GOOD
  promote-model:
    runs-on: ubuntu-latest
    needs: [training, evaluate-model]
    if: needs.evaluate-model.result == 'success'
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: 📦 Install MLflow
      run: pip install mlflow boto3
    
    - name: 🏆 Promote model to staging
      run: |
        python << 'EOF'
        import mlflow
        import os
        
        try:
            mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])
            client = mlflow.MlflowClient()
            
            model_name = "loan-default-model"
            
            # Get latest model
            latest_versions = client.get_latest_versions(model_name, stages=["None"])
            
            if not latest_versions:
                print("❌ No models found for promotion")
                exit(1)
            
            latest_model = latest_versions[0]
            run = client.get_run(latest_model.run_id)
            metrics = run.data.metrics
            
            accuracy = metrics.get('accuracy', 0)
            f1_score = metrics.get('f1_score', 0)
            
            print(f"🔍 Evaluating model v{latest_model.version} for promotion")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   F1-Score: {f1_score:.4f}")
            
            # Promotion logic based on quality
            if accuracy >= 0.75 and f1_score >= 0.70:
                # Promote to Staging
                client.transition_model_version_stage(
                    name=model_name,
                    version=latest_model.version,
                    stage="Staging",
                    archive_existing_versions=True
                )
                print(f"✅ Model v{latest_model.version} promoted to Staging")
                
                # If really good, consider for production
                if accuracy >= 0.80 and f1_score >= 0.75:
                    print(f"🎯 Model quality excellent - candidate for production promotion")
                    
            else:
                print(f"⚠️  Model quality below promotion threshold")
                print(f"   Required: Accuracy ≥ 0.75, F1 ≥ 0.70")
                print(f"   Actual:   Accuracy = {accuracy:.4f}, F1 = {f1_score:.4f}")
                
        except Exception as e:
            print(f"❌ Model promotion failed: {e}")
        EOF

  # TRIGGER DEPLOYMENT PIPELINE
  trigger-deployment:
    runs-on: ubuntu-latest
    needs: [training, evaluate-model, promote-model]
    if: needs.promote-model.result == 'success'
    
    steps:
    - name: 🚀 Trigger deployment pipeline
      run: |
        echo "🚀 Training pipeline completed successfully!"
        echo "📊 Model has been trained and evaluated"
        echo "🎯 Deployment pipeline will be triggered automatically"
        echo ""
        echo "📋 Training Summary:"
        echo "   ✅ Data validation passed"
        echo "   ✅ Model training completed"  
        echo "   ✅ Model evaluation completed"
        echo "   ✅ Model promoted to staging"
        echo ""
        echo "⏭️  Next: Model Deployment pipeline will validate and deploy the model"

  # NOTIFICATION
  notify:
    runs-on: ubuntu-latest
    needs: [validate-data, training, evaluate-model, promote-model, trigger-deployment]
    if: always()
    
    steps:
    - name: 📊 Determine training status
      id: status
      run: |
        if [[ "${{ needs.training.result }}" == "success" && "${{ needs.evaluate-model.result }}" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "message=✅ Training pipeline completed successfully" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.validate-data.result }}" == "failure" ]]; then
          echo "status=data_failed" >> $GITHUB_OUTPUT
          echo "message=❌ Data validation failed" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.training.result }}" == "failure" ]]; then
          echo "status=training_failed" >> $GITHUB_OUTPUT
          echo "message=❌ Model training failed" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.evaluate-model.result }}" == "failure" ]]; then
          echo "status=evaluation_failed" >> $GITHUB_OUTPUT
          echo "message=❌ Model evaluation failed" >> $GITHUB_OUTPUT
        else
          echo "status=unknown" >> $GITHUB_OUTPUT
          echo "message=❓ Training pipeline completed with unknown status" >> $GITHUB_OUTPUT
        fi
    
    - name: 📢 Send training notification
      run: |
        echo "📢 Training Pipeline Status: ${{ steps.status.outputs.message }}"
        echo "🧪 Experiment: ${{ github.event.inputs.experiment_name || 'automated-training' }}"
        echo "📊 Data Hash: ${{ needs.validate-data.outputs.data-hash }}"
        echo "👤 Triggered by: ${{ github.actor }}"
        echo "🔗 Workflow: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        echo ""
        if [[ "${{ steps.status.outputs.status }}" == "success" ]]; then
          echo "⏭️  Next: Deployment pipeline will be triggered automatically"
        fi